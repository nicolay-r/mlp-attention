## Multilayer Perceptron Attention
![](https://img.shields.io/badge/Python-3.6-brightgreen.svg)
![](https://img.shields.io/badge/TensorFlow-1.14.0-yellowgreen.svg)

> **UPD December 7rd, 2019:** this attention model becomes a part of 
[AREkit](https://github.com/nicolay-r/AREkit) framework.


This project is an unofficial implementation of MLP attention -- multilayer perceptron 
attention network, proposed by Yatian Shen and Xuanjing Huang 
as an application for **Relation Extraction Task**
[[parper](https://www.aclweb.org/anthology/C16-1238)].

![alt text](images/attention.png)

Vector representation of words and entities includes:
* Term embedding;
* Part-Of-Speech (POS) embedding;
* Distance embedding;

## References
* Attention-Based Convolutional Neural Network for Semantic Relation Extraction [[paper]](http://www.aclweb.org/anthology/C16-1238)
	* Yatian Shen and Xuanjing Huang
	* COLING 2016
